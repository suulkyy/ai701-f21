{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI701_hw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3yxMfsRi05g"
      },
      "source": [
        "# **2021 Fall AI701 Homework assignment 3**\n",
        "Read the following problem sheet and submit your completed $\\texttt{ipynb}$ file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCnmBS4vlEUX"
      },
      "source": [
        "## Problem 1 (70 pts)\n",
        "\n",
        "We will going to implement the sequential importance sampling (SIS) and sequential importance resampling (SIR) for a simple state-space model. The state-space model is defined as follows:\n",
        "\n",
        "$\\displaystyle x_1 \\sim \\mathcal{N}\\bigg(\\mu, \\frac{\\sigma^2}{1 - \\rho^2}\\bigg).$\n",
        "\n",
        "$\\displaystyle x_t|x_{t-1} \\sim \\mathcal{N}(\\mu + \\rho(x_{t-1} - \\mu), \\sigma^2).$\n",
        "\n",
        "$\\displaystyle y_t|x_t \\sim \\mathcal{N}(0, e^{x_t}).$\n",
        "\n",
        "The following codes define the functions required for generating sequences from this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-6gnROcAVBA"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import logsumexp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLj4HkLfIVt-"
      },
      "source": [
        "# parameters, we will use a fixed set of parameters throughout this homework.\n",
        "params = {'mu':-1.0, 'rho':0.95, 'sigma':0.2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hklJGUOCH6XV"
      },
      "source": [
        "# sample x_1 ~ p(x_1).\n",
        "def sample_x1(num_samples=1):\n",
        "    std = params['sigma'] / np.sqrt(1 - params['rho']**2)    \n",
        "    return params['mu'] + std*npr.normal(size=num_samples)\n",
        "        \n",
        "# sample x_t ~ p(x_t|x_{t-1}) with x_prev denoting x_{t-1}.\n",
        "def sample_x(x_prev):\n",
        "    mean = params['mu'] + params['rho']*(x_prev - params['mu'])\n",
        "    shape = None if type(x_prev)==float else x_prev.shape\n",
        "    return mean + params['sigma']*npr.normal(size=shape)\n",
        "\n",
        "# sample y_t ~ p(y_t|x_t).\n",
        "def sample_y(x):    \n",
        "    shape = None if type(x)==float else x.shape\n",
        "    return np.exp(0.5*x)*npr.normal(size=shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2G2ZpDel8T2"
      },
      "source": [
        "Using these functions, the following code actually generates a sequence $\\{y_t\\}_{t=1}^T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOIhwiFGLdV1"
      },
      "source": [
        "T = 100\n",
        "x_true = np.zeros(T)\n",
        "y = np.zeros(T)\n",
        "npr.seed(42)\n",
        "\n",
        "x_true[0] = sample_x1()\n",
        "y[0] = sample_y(x_true[0])\n",
        "for t in range(1, T):\n",
        "    x_true[t] = sample_x(x_true[t-1])\n",
        "    y[t] = sample_y(x_true[t])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
        "axes[0].set_title(r'$x_{true}$')\n",
        "axes[0].plot(range(T), x_true)\n",
        "axes[1].set_title(r'$y$')\n",
        "axes[1].plot(range(T), y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnrDn2UBD-Ib"
      },
      "source": [
        "To define sequential importance sampling algorithm, we choose the simplest possible proposal distribution; the prior transition distribution.\n",
        "$\n",
        "q(x_t | x_{1:t-1}, y_{1:t}) = p(x_t | x_{t-1}).\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7ZEl7o7Uobv"
      },
      "source": [
        "# propose a sample x_t ~ q(x_t|x_{1:t-1}, y_{1:t}) = p(x_t|x_{t-1})\n",
        "def propose_x(x_prev):\n",
        "    return sample_x(x_prev)\n",
        "\n",
        "# computes the log-likelihood p(y_t|x_t)\n",
        "def log_likel(y, x):\n",
        "    var = np.exp(x)\n",
        "    ll = -0.5*y**2/var - 0.5*np.log(2*np.pi*var)\n",
        "    return ll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q07TmVNmuLl"
      },
      "source": [
        "Now we are ready to write the sequential importance sampling algorithm. Let's first start by writing the function to initialize set of particles at $t=1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYGNun9SnvRX"
      },
      "source": [
        "# initialize num_particles number of particles.\n",
        "# particles are represented with a dictionary:\n",
        "# 'x' denotes the actual samples of the states with shape (num_particles) * (num_timesteps).\n",
        "# 'log_w' denotes log of unnormalized importance weights with shape (num_particles) * (num_timesteps).\n",
        "# After initialization, particles['x'] and particles['log_w'] should have shape (num_particles) * 1.\n",
        "def init_particles(y, num_particles):\n",
        "    x = sample_x1(num_samples=num_particles)\n",
        "    log_w = log_likel(y, x)\n",
        "    particles = {'num_particles':num_particles, \n",
        "                 'x':x[:,None], \n",
        "                 'log_w':log_w[:,None]}\n",
        "    return particles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW5arpPuoXtJ"
      },
      "source": [
        "(a) (20pts) Complete the following code to extend the set of particles for a new observation $y_t$. Taking the set of particles constructed up to time $t-1$ and $y_t$, the function should output update particles with states and unnomarlized importance weights constructed up to time $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1uoxVT69Oy3"
      },
      "source": [
        "# particles['x'] and particles['log_w'] have shape (num_particles) * (t-1)\n",
        "# given y_t, the following function should return the update set of particles\n",
        "# where both particles['x'] and particles['log_w'] have shape (num_particles) * t\n",
        "def SIS_extend_particles(y, particles):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        " \n",
        "    ######################################################################\n",
        "    return particles\n",
        "\n",
        "# Once you have the function to extend particles, the SIS algorithm is straightforward as follows.\n",
        "def run_SIS(y, num_particles):\n",
        "    particles = init_particles(y[0], num_particles)\n",
        "    for t in range(1, len(y)):\n",
        "        particles = SIS_extend_particles(y[t], particles)\n",
        "    return particles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3PFFYNOpQQm"
      },
      "source": [
        "(b) (10pts) Complete the following functions computing the estimate of the log marginal $\\log p(y_{1:T})$ and the effective sample size using the particles constructed up to time $T$. \n",
        "\n",
        "Hint: use $\\texttt{logsumexp}$ function for numerical stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D65jWSrTpB_b"
      },
      "source": [
        "# return a scalar value computing the estimate of the log marginal likelihood log p(y_{1:T})\n",
        "def SIS_log_marginal(particles):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "\n",
        "    ######################################################################\n",
        "\n",
        "# return a scalar value computing the effective sample size \n",
        "def compute_ess(particles):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "  \n",
        "    ######################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiekMLfXpv5o"
      },
      "source": [
        "(c) (10pts) Since we have the true states $x_{\\text{true}}$ that we have used for the data generation, we can compute the estimation error for the estimatated states. That is, computing the expected error defined as\n",
        "\n",
        "$\n",
        "\\displaystyle \\mathcal{E}(x_\\text{true}) = \\frac{1}{T} \\mathbb{E}_{p(x_{1:T}|y_{1:T})}\\bigg[ \\sum_{t=1}^T (x_t - x_{\\text{true}, t})^2\\bigg] = \\frac{1}{T} \\int \\sum_{t=1}^T (x_t - x_{\\text{true}, t})^2 p(x_{1:T}|y_{1:T}) \\mathrm{d} x_{1:T}.\n",
        "$\n",
        "\n",
        "Complete the following code computing the approximation of this expected error using the particles and importance weights we construct with SIS. Then run the complete codes to get the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI4MYU_ypxN3"
      },
      "source": [
        "# return a scalar estimate of the expected error\n",
        "def estimate_error(x_true, particles):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "          \n",
        "    ######################################################################\n",
        "\n",
        "num_particles = 200\n",
        "particles = run_SIS(y, num_particles)\n",
        "\n",
        "lm = SIS_log_marginal(particles)\n",
        "ess = compute_ess(particles)\n",
        "err = estimate_error(x_true, particles)\n",
        "print(f'SIS log marginal {lm:.4f}, ESS {ess:.4f}, expected error {err:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gVECF9DrdbH"
      },
      "source": [
        "(d) (5pts) For visualization, we will compute the mean and variances of the estimated states.\n",
        "\n",
        "$\n",
        "\\displaystyle \n",
        "\\mathbb{E}_{p(x_{1:T}|y_{1:T})}[x_t] = \\int x_t p(x_{1:T}|y_{1:T}) \\mathrm{d}x_{1:T}, \\quad\n",
        "\\mathrm{Var}_{p(x_{1:T}|y_{1:T})}[x_t] = \\int (x_t - \\mathbb{E}_{p(x_{1:T}|y_{1:T})})^2 p(x_{1:T}|y_{1:T}) \\mathrm{d}x_{1:T}.\n",
        "$\n",
        "\n",
        "Complete the following codes computing the approximation of expected means and variances using the particles and importance weights we get from SIS. Run the visualization code to see if the algorithm works well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Fb0YorsUPq"
      },
      "source": [
        "# return a pair of T-dim vectors estimating the mean and variance of the sequence x_{1:T}\n",
        "def estimate_x_mean_and_var(particles):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "\n",
        "    ######################################################################\n",
        "    return x_mean, x_var\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.title('SIS estimated states')\n",
        "x_mean, x_var = estimate_x_mean_and_var(particles)\n",
        "x_std = np.sqrt(x_var)\n",
        "plt.fill_between(range(T), x_mean-x_std, x_mean+x_std, \n",
        "                 facecolor='b', edgecolor=None, alpha=0.2)\n",
        "plt.plot(range(T), x_mean, color='b', label=r'$x_{mean}$')\n",
        "plt.plot(range(T), x_true, '--r', label=r'$x_{true}$', )\n",
        "plt.legend(fontsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhFqQzNKsbRo"
      },
      "source": [
        "(e) (25pts) Complete the following functions required for sequential importance resampling. You may use $\\texttt{npr.choice}$ function for categorical resampling step. Run the algorithm to verify your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Uu39dZxUBN"
      },
      "source": [
        "# particles['x'] and particles['log_w'] have shape (num_particles) * (t-1)\n",
        "# given y_t, the following function should return the update set of particles\n",
        "# where both particles['x'] and particles['log_w'] have shape (num_particles) * t\n",
        "def SIR_extend_particles(y, particles):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "             \n",
        "    ######################################################################\n",
        "    return particles\n",
        "\n",
        "def run_SIR(y, num_particles):\n",
        "    particles = init_particles(y[0], num_particles)\n",
        "    for t in range(1, len(y)):\n",
        "        particles = SIR_extend_particles(y[t], particles)\n",
        "    return particles\n",
        "\n",
        "# return a scalar value computing the estimate of the log marginal likelihood log p(y_{1:T})\n",
        "def SIR_log_marginal(particles):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "    \n",
        "    ######################################################################\n",
        "\n",
        "num_particles = 200\n",
        "particles = run_SIR(y, num_particles)\n",
        "\n",
        "lm = SIR_log_marginal(particles)\n",
        "ess = compute_ess(particles)\n",
        "err = estimate_error(x_true, particles)\n",
        "print(f'SIR log marginal {lm:.4f}, ESS {ess:.4f}, expected error {err:.4f}')\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.title('SIR estimated states')\n",
        "x_mean, x_var = estimate_x_mean_and_var(particles)\n",
        "x_std = np.sqrt(x_var)\n",
        "plt.fill_between(range(T), x_mean-x_std, x_mean+x_std, \n",
        "                 facecolor='b', edgecolor=None, alpha=0.2)\n",
        "plt.plot(range(T), x_mean, color='b', label=r'$x_{mean}$')\n",
        "plt.plot(range(T), x_true, '--r', label=r'$x_{true}$', )\n",
        "plt.legend(fontsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8sEk0DpuqK9"
      },
      "source": [
        "## Problem 2 (30pts)\n",
        "Consider sampling from the following target distribution,\n",
        "\n",
        "$$\n",
        "p(x) \\propto \\exp( - U(x)), \\quad U(x) = \\frac{1}{2} x^\\top A x, \\quad A = \\begin{bmatrix} 2 & 1.2 \\\\ 1.2 & 2 \\end{bmatrix}, \n",
        "$$\n",
        "\n",
        "where $x \\in \\mathbb{R}^2$ and $A \\in \\mathbb{R}^{2\\times 2}$. We can visualize the contour plot of the density function as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMNj19Itup3F"
      },
      "source": [
        "A = np.array([[2, 1.2], [1.2, 2]])\n",
        "# x: 2 dimensional vector or N times 2 dimensional matrix.\n",
        "def energy_function(x):    \n",
        "    if x.ndim == 1:\n",
        "        return 0.5 * np.inner(x, np.dot(A, x)) \n",
        "    elif x.ndim == 2:\n",
        "        return 0.5 * (x * np.dot(x, A)).sum(-1)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "def plot_density(alpha=1.0):\n",
        "    nx, ny = 50, 50\n",
        "    x = np.linspace(-3, 3, nx)\n",
        "    y = np.linspace(-3, 3, ny)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    z = np.exp(-energy_function(np.concatenate([xx.reshape((-1, 1)), yy.reshape((-1, 1))], -1)))\n",
        "    plt.contour(x, y, z.reshape((nx, ny)), cmap='inferno', alpha=alpha)\n",
        "\n",
        "plot_density()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyCMk24mx1l3"
      },
      "source": [
        "We will sample from this target distribution using Hamiltonian Monte Carlo. The kinetic energy function and Hamiltonian function are assumed to be\n",
        "\n",
        "$\n",
        "K(p) = \\frac{1}{2}p^\\top p, \\quad H(x, p) = U(x) + K(p).\n",
        "$\n",
        "\n",
        "Complete the following codes running HMC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2kqy6NzyBs-"
      },
      "source": [
        "# computes grad_x U(x) for 2 dimensional vector x.\n",
        "def energy_function_grad(x):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "    \n",
        "    ######################################################################\n",
        "\n",
        "# compute the kinetic energy for 2 dimensional momentum vector p.\n",
        "def kinetic_energy_function(p):\n",
        "    return 0.5 * np.inner(p, p)\n",
        "\n",
        "# run HMC and collect (num_samples) number of samples.\n",
        "# x0: initial position, 2 dimensional vector.\n",
        "# eps: step size\n",
        "# L: number of leapfrog integrations\n",
        "# output: (num_samples) * 2 dimensional matrix collecting the samples.\n",
        "def run_HMC(x0, eps, L, num_samples):\n",
        "    # complete this part\n",
        "    ######################################################################\n",
        "   \n",
        "    ######################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWouTbXd4QS4"
      },
      "source": [
        "Run the following code the verify your sampler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk46P4GM02En"
      },
      "source": [
        "x0 = npr.rand(2)\n",
        "eps = 0.01\n",
        "L = 30\n",
        "num_samples = 5000\n",
        "samples = run_HMC(x0, eps, L, num_samples)\n",
        "plot_density()\n",
        "plt.scatter(samples[:,0], samples[:,1], alpha=0.5, s=20, color='k')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}