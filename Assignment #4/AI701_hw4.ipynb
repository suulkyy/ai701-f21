{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI701_hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_dae2XVwRmg"
      },
      "source": [
        "# **2021 Fall AI701 Homework assignment 4**\n",
        "Read the following problem sheet and submit your completed $\\texttt{ipynb}$ file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlsEkkW6wiR8"
      },
      "source": [
        "In this homework, we will implement a simple Bayesian neural network (BNN) using $\\texttt{jax}$. If you are not familiar with $\\texttt{jax}$, you can treat it as a $\\texttt{numpy}$ equipped with automatic differentiation (plus many other useful functionalities). This homework is designed to minimize your effort to get familiar with $\\texttt{jax}$, but if you still find it difficult, you may refer to a tutorial (https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Wji8TOVaXn"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install optax\n",
        "import optax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G43Y3R2xeuq"
      },
      "source": [
        "$\\texttt{jax.numpy}$ includes most of the functions implemented in $\\texttt{numpy}$, so you can almost treat it as original $\\texttt{numpy}$. $\\texttt{optax}$ is an additional library including implementation for various optimizers. \n",
        "\n",
        "In this homework, we will solve a regression problem with BNNs. For that, we first generate training data, where the pair $(x_\\text{train}, y_\\text{train})$ is generated from some underlying function with Gaussian measurement noises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVFlrWBOWHDp"
      },
      "source": [
        "def true_function(x):\n",
        "    return x * jnp.sin(3*jnp.pi*x)\n",
        "    \n",
        "key = jax.random.PRNGKey(42)\n",
        "num_train = 100\n",
        "obs_std = 0.3\n",
        "train_x = jnp.concatenate([jnp.linspace(-0.8, -0.2, num_train//2), jnp.linspace(0.2, 0.8, num_train//2)])\n",
        "key, key_ = jax.random.split(key, 2)\n",
        "train_y = true_function(train_x) + obs_std * jax.random.normal(key_, [num_train])\n",
        "plt.plot(train_x, train_y, '.b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At0FeLKQyFGR"
      },
      "source": [
        "One thing you should notice in the above code is that we are explicitly keeping track of the random seed ($\\texttt{key}$) for random number generation. This is one property of $\\texttt{jax}$ different from $\\texttt{numpy}$, and helps us to enhance reproducibility. Whenever you want to generate something randomly, you should provide a $\\texttt{key}$ for that.\n",
        "\n",
        "Now we proceed to define a neural network architecture for our regression problem. We use a simple multi-layer perceptron (MLP) having two hidden layers. The codes below define the function to do forward pass through MLP and the function to initialize the parameters of MLP. The MLP takes an input $x$ and outputs the predicted function value $\\mu(x;\\theta)$ and its observation noise $\\sigma(x;\\theta)$ where $\\theta$ is the parameter of the MLP.\n",
        "\n",
        "Parameters of MLP are represented by a dictionary $\\texttt{string} \\to \\texttt{jnp.ndarray}$.  $\\texttt{W1}$ and $\\texttt{b1}$ represent the weight matrix and bias vector of the first layer, and the rest should be clear. Note also how the code is using keys for random initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gioYezfrZI7l"
      },
      "source": [
        "def MLP(params, x):\n",
        "    y = jax.nn.swish(jnp.dot(x[...,None], params['W1']) + params['b1'])\n",
        "    y = jax.nn.swish(jnp.dot(y, params['W2']) + params['b2'])\n",
        "    y = jnp.dot(y, params['W3']) + params['b3']\n",
        "    y_mean = y[...,0]\n",
        "    y_std = jax.nn.softplus(y[...,1])\n",
        "    return y_mean, y_std\n",
        "\n",
        "def init_params(key, num_hidden):\n",
        "    W_init = jax.nn.initializers.kaiming_normal()\n",
        "    b_init = jax.nn.initializers.zeros\n",
        "\n",
        "    key, key_ = jax.random.split(key, 2)\n",
        "    keys = jax.random.split(key_, 6)    \n",
        "    W1 = W_init(keys[0], (1, num_hidden))\n",
        "    b1 = b_init(keys[1], (num_hidden,))\n",
        "    W2 = W_init(keys[2], (num_hidden, num_hidden))\n",
        "    b2 = b_init(keys[3], (num_hidden,))\n",
        "    W3 = W_init(keys[4], (num_hidden, 2))\n",
        "    b3 = b_init(keys[5], (2,))\n",
        "\n",
        "    return key, {'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2, 'W3':W3, 'b3':b3}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAqUH--Mz4CC"
      },
      "source": [
        "Now we define the likelihood and prior for Bayesian inference. The likelihood is simply given as a Gaussian,\n",
        "\n",
        "$$\n",
        "p(y|x, \\theta) = \\mathcal{N}(y | \\mu(x;\\theta), \\sigma^2(x;\\theta)).\n",
        "$$\n",
        "\n",
        "For the prior, we assume i.i.d. zero-mean Gaussian prior.\n",
        "\n",
        "$$\n",
        "p(\\theta) = \\mathcal{N}(\\theta ; 0, \\mathrm{diag}(\\lambda^2)),\n",
        "$$\n",
        "\n",
        "where $\\lambda$ is a hyperparameter for prior standard deviation. Given a training set $(X, Y) = \\{(x_i, y_i)\\}_{i=1}^n$, the joint likelihood is then computed as\n",
        "\n",
        "$$\n",
        "\\displaystyle\n",
        "p(Y, \\theta | X) = p(\\theta)\\prod_{i=1}^n p(y_i|x_i, \\theta).\n",
        "$$\n",
        "\n",
        "As an illustraion, let us first solve the regression via Maximum a Posterior (MAP). We need to minimize the negative log joint likelihood,\n",
        "\n",
        "$$\n",
        "U(\\theta) = -\\log p(\\theta) - \\sum_{i=1}^n \\log p(y_i|x_i, \\theta),\n",
        "$$\n",
        "and the codes below defines the functions computing the negative log joint likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct7ESB7XdJ1q"
      },
      "source": [
        "# computes -log p(y|x, params) except for the constants not depending on the parameters.\n",
        "def neg_log_likel(params, x, y):\n",
        "    y_mean, y_std = MLP(params, x)\n",
        "    nll = 0.5 * (y - y_mean)**2 / y_std**2 + jnp.log(y_std) \n",
        "    return nll.sum()\n",
        "\n",
        "# computes -log p(params) except for the constants not depending on the parameters.\n",
        "def neg_log_prior(params, prior_std):    \n",
        "    def nlp(x):\n",
        "        return jnp.sum(0.5 * x.reshape(-1)**2 / prior_std**2)\n",
        "    return sum(jax.tree_leaves(jax.tree_map(nlp, params)))\n",
        "\n",
        "# computes -log p(y, params | x) except for the constants not depending on the parameters.\n",
        "def neg_log_joint(params, x, y, prior_std):\n",
        "    return neg_log_likel(params, x, y) + neg_log_prior(params, prior_std)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj9qNCiA2Ifq"
      },
      "source": [
        "What we have to do now is to compute the gradient of the $U(\\theta)$ with $\\theta$ for gradient descent. Using $\\texttt{jax}$, this can be done very easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUEYrvPa2HVr"
      },
      "source": [
        "def loss_and_grad(params, x, y, prior_std):\n",
        "    return jax.value_and_grad(neg_log_joint, argnums=0)(params, x, y, prior_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21W-5L7R2ctj"
      },
      "source": [
        "The function above simply transforms our objective function into another function computing the loss values ($U$) and its gradient w.r.t. the parameter $(\\nabla_\\theta U(\\theta))$. \n",
        "\n",
        "Throughout this homework, we will use fixed values for the hyperparameters (number of hidden nodes in MLP and prior standard deviation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odaQD2zhOGMF"
      },
      "source": [
        "# hyperparameters\n",
        "num_hidden = 32\n",
        "prior_std = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDc04SVp22HT"
      },
      "source": [
        "The code below  defines a training step of MAP procedure. The function $\\texttt{train_step}$ takes the current parameters and optimizer state, does gradient descent, and returns the updated parameters, optimizer state, and loss value. $\\texttt{jit}$ stands for just-in-time compilation, which makes the computation much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCywY_0nOJVF"
      },
      "source": [
        "opt = optax.adam(1e-3)\n",
        "@jax.jit\n",
        "def train_step(params, opt_state):\n",
        "    loss, grads = loss_and_grad(params, train_x, train_y, prior_std)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return loss, params, opt_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuk5PO8H3sa8"
      },
      "source": [
        "Given the training step function, the MAP estimation is simple as below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om3XvoNWjMZc"
      },
      "source": [
        "key, params = init_params(key, num_hidden)\n",
        "opt_state = opt.init(params)\n",
        "num_steps = 5000\n",
        "for t in range(1, num_steps+1):\n",
        "    loss, params, opt_state = train_step(params, opt_state)\n",
        "    if t % 500 == 0:\n",
        "        print(f'step {t}, loss {loss:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTXGC_w-3zRT"
      },
      "source": [
        "The code below test the trained MLP on the interval $[-1.1, 1.1]$. The blue line shows the mean prediction $\\mu(x)$, and the shaded area visualizes $\\pm\\sigma(x)$ and $\\pm 2 \\sigma(x)$ interval. As you can see, the MAP estimation fits the model quite well, but it fails to properly capture the uncertainty, especially in the region where training data are scarse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYTj4HkK7_Aj"
      },
      "source": [
        "test_x = jnp.linspace(-1.1, 1.1, 100)\n",
        "y_mean, y_std = MLP(params, test_x)\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.fill_between(test_x, y_mean-2*y_std, y_mean+2*y_std, alpha=0.2, facecolor='skyblue', edgecolor=None)\n",
        "plt.fill_between(test_x, y_mean-y_std, y_mean+y_std, alpha=0.4, facecolor='skyblue', edgecolor=None)\n",
        "plt.plot(test_x, y_mean, label='prediction')\n",
        "plt.plot(train_x, train_y, 'rx', label='training data')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PwRjKAm4SX1"
      },
      "source": [
        "## Problem 1 (40pts)\n",
        "In this problem, we will implement HMC for the given regression problem. Consider the Hamiltonian function defined as follows:\n",
        "\n",
        "$$\n",
        "H(\\theta, p) = U(\\theta) + K(p),\n",
        "$$\n",
        "where\n",
        "$$\n",
        "U(\\theta) = -\\sum_{i=1}^n \\log p(y_i|x_i,\\theta) - \\log p(\\theta), \\quad \n",
        "K(p) = \\frac{1}{2}p^\\top p.\n",
        "$$\n",
        "That is, the potential energy is the negative log joint likelihood function defined above and the kinetic energy function is negative log density of standard Gaussian.\n",
        "\n",
        "(a) (30pts) Fill the blank in the below code for a leapfrog step in HMC. Note that both parameters and momentums are represented as dictionaries; elementry operations such as addition or scalar multiplication may not be straightforward. You can use $\\texttt{jax.tree_multimap}$ for this. For instance, say you have two dictionaries $\\texttt{dict1}$ and $\\texttt{dict2}$ having the same keys. You want to multiply $4.0$ to the values of the first dictionary, multiply $2.0$ to the values of the second dictionary and add them to make a new dictionary. This operation be implemented as follows:\n",
        "\n",
        "$$\n",
        "\\texttt{result = jax.tree_multimap(lambda x1, x2: 4.0*x1 + 2.0*x2, dict1, dict2)}.\n",
        "$$\n",
        "\n",
        "For the first argument, you pass the function that you want to apply for each members of dictionaries. For the rest of the arguments, you pass the dictionaries that you will be applying the first argument. For more examples, see https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87SOGMdASTaK"
      },
      "source": [
        "def generate_momentum(key, params):    \n",
        "    momentum = {}\n",
        "    for k, v in params.items():\n",
        "        key, key_ = jax.random.split(key, 2)\n",
        "        momentum[k] = jax.random.normal(key_, v.shape) \n",
        "    return key, momentum\n",
        "\n",
        "@jax.jit\n",
        "def HMC_step(key, params, eps, num_leapfrog_steps):   \n",
        "\n",
        "    # Leapfrog step with step size eps.\n",
        "    # takes the current (parameter, momentum, U, grads), do single leapfrog step,\n",
        "    # and returns the updated (parameters, momentum, U, grads).\n",
        "    def leapfrog_body(_, carry):\n",
        "        params, momentum, U, grads = carry\n",
        "\n",
        "        # fill this part\n",
        "        ######################################################################\n",
        "\n",
        "        ######################################################################\n",
        "        \n",
        "        return (params, momentum, U, grads)\n",
        "\n",
        "    # initialize momentum\n",
        "    key, momentum = generate_momentum(key, params)\n",
        "    U, grads = loss_and_grad(params, train_x, train_y, prior_std)\n",
        "    K = sum([0.5*jnp.sum(p**2) for p in jax.tree_leaves(momentum)])\n",
        "    H = U + K    \n",
        "\n",
        "    # propose new sample via leapfrog integration\n",
        "    new_params, new_momentum, new_U, new_grads = jax.lax.fori_loop(\n",
        "        0, num_leapfrog_steps, leapfrog_body, (params, momentum, U, grads))\n",
        "    new_K = sum([0.5*jnp.sum(p**2) for p in jax.tree_leaves(new_momentum)])\n",
        "    new_H = new_U + new_K\n",
        "\n",
        "    # compute acceptance probability    \n",
        "    accept_prob = jnp.exp(jnp.minimum(0.0, -new_H + H))\n",
        "    key, key_ = jax.random.split(key, 2)\n",
        "    accepted = jax.random.uniform(key_) < accept_prob\n",
        "\n",
        "    # MH accept-reject step\n",
        "    (params, H, U, K) = jax.lax.cond(accepted, \n",
        "                                     lambda _: (new_params, new_H, new_U, new_K),\n",
        "                                     lambda _: (params, H, U, K),\n",
        "                                    operand=None)\n",
        "    \n",
        "    return key, accepted, params, H, U, K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JoDSpSup50"
      },
      "source": [
        "Having completed the code above, we can run HMC using the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TCgP4sNuvB8"
      },
      "source": [
        "def HMC(key, eps, num_leapfrog_steps, num_steps, burn_in, thin):\n",
        "\n",
        "    # initialize params\n",
        "    key, params = init_params(key, num_hidden)\n",
        "    samples = []\n",
        "    num_accepted = 0\n",
        "    for t in range(1, num_steps + 1):\n",
        "        key, accepted, params, H, U, K = HMC_step(key, params, eps, num_leapfrog_steps)\n",
        "        num_accepted += int(accepted)\n",
        "        if t % 100 == 0:            \n",
        "            nll = neg_log_likel(params, train_x, train_y)\n",
        "            nlp = neg_log_prior(params, prior_std)\n",
        "            print((\n",
        "                f'step {t}, acc rate {num_accepted/t:.3f}, '\n",
        "                f'H {H:.4f}, U {U:.4f}, K {K:.4f}, '\n",
        "                f'nll {nll:.4f}, nlp {nlp:.4f}'\n",
        "            ))\n",
        "\n",
        "        if t > burn_in and t % thin == 0:\n",
        "            samples.append(params)\n",
        "\n",
        "    return samples\n",
        "\n",
        "eps = 5e-4\n",
        "num_leapfrog_steps = 100\n",
        "num_steps = 3000\n",
        "burn_in = 1500\n",
        "thin = 10\n",
        "samples = HMC(key, eps, num_leapfrog_steps, num_steps, burn_in, thin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yQhZVs62YAK"
      },
      "source": [
        "(b) (10pts) Having completed the code, run HMC to get samples. For visualization, we want to approximate the predictive means and variances for given test data. Complete the following code computing predictive means and variances using the samples collected from HMC. Hint: for variance, keep in mind the law of total variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T4Z0Eu6x7gH"
      },
      "source": [
        "def compute_pred_mean_and_std(means, stds):\n",
        "     # complete this part\n",
        "    ###################################################################\n",
        "\n",
        "    ###################################################################\n",
        "    return pred_mean, pred_std\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(121)\n",
        "test_x = jnp.linspace(-1.1, 1.1, 100)\n",
        "y_means, y_stds = [], []\n",
        "for params in samples:\n",
        "    y_mean, y_std = MLP(params, test_x)\n",
        "    plt.plot(test_x, y_mean, alpha=0.2)    \n",
        "    y_means.append(y_mean)\n",
        "    y_stds.append(y_std)\n",
        "plt.plot(train_x, train_y, 'rx', label='training data')\n",
        "plt.legend()\n",
        "\n",
        "y_means = jnp.stack(y_means)\n",
        "y_stds = jnp.stack(y_stds)\n",
        "y_mean, y_std = compute_pred_mean_and_std(y_means, y_stds)\n",
        "plt.subplot(122)\n",
        "plt.fill_between(test_x, y_mean-2*y_std, y_mean+2*y_std, alpha=0.2, facecolor='skyblue', edgecolor=None)\n",
        "plt.fill_between(test_x, y_mean-y_std, y_mean+y_std, alpha=0.4, facecolor='skyblue', edgecolor=None)\n",
        "plt.plot(test_x, y_mean, label='prediction')\n",
        "plt.plot(train_x, train_y, 'rx', label='training data')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHOwewHq25Dk"
      },
      "source": [
        "# Problem 2 (60pts)\n",
        "\n",
        "Now we are going to solve the same regression problem using Bayes-by-Backprop (BBB). We assume the simplest possible mean-field Gaussian variational distribution for the parameters.\n",
        "\n",
        "$$\n",
        "q(\\theta;\\phi) = \\mathcal{N}(\\theta ; \\mu_\\phi, \\mathrm{diag}(\\sigma_\\phi^2)).\n",
        "$$\n",
        "\n",
        "(a) (5pts) Complete the following code initializing the variationa parameters $\\phi$. The code should look similar to $\\texttt{init_params}$ function above, but now we have more parameters to initialize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOhxJCNo4Rpy"
      },
      "source": [
        "def init_var_params(key, num_hidden):\n",
        "    # sigma_uc is the unconstrained transform of sigma such that sigma = softplus(sigma_uc).    \n",
        "    W_mu_init = jax.nn.initializers.kaiming_normal()\n",
        "    W_sigma_uc_init = lambda key, shape: -3.0 + 0.1 * jax.random.normal(key, shape)\n",
        "    b_mu_init = jax.nn.initializers.zeros\n",
        "    b_sigma_uc_init = lambda key, shape: -3.0 + 0.1 * jax.random.normal(key, shape)\n",
        "\n",
        "    # fill this part\n",
        "    ################################################################################\n",
        "\n",
        "    ################################################################################\n",
        "\n",
        "    var_params = {'W1_mu':W1_mu, 'W1_sigma_uc':W1_sigma_uc,\n",
        "               'b1_mu':b1_mu, 'b1_sigma_uc':b1_sigma_uc, \n",
        "               'W2_mu':W2_mu, 'W2_sigma_uc':W2_sigma_uc,\n",
        "               'b2_mu':b2_mu, 'b2_sigma_uc':b2_sigma_uc,\n",
        "               'W3_mu':W3_mu, 'W3_sigma_uc':W3_sigma_uc,\n",
        "               'b3_mu':b3_mu, 'b3_sigma_uc':b3_sigma_uc\n",
        "               }\n",
        "    return key, var_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhWxnqHDVFWy"
      },
      "source": [
        "(b) (15pts) The objective function for the variational inference is the Evidence LOwer Bound (ELBO), which is computed as follows:\n",
        "\n",
        "$$\n",
        "\\displaystyle\n",
        "\\mathcal{L}(\\phi) = \\sum_{i=1}^n \\mathbb{E}_{q(\\theta;\\phi)}[\\log p(y_i|x_i, \\theta)] - \\gamma\\cdot D_\\text{KL}[ q(\\theta;\\phi) \\Vert p(\\theta)].\n",
        "$$\n",
        "\n",
        "(Note that we are introducing additional scaling parameter $\\gamma$ to control the strength of the KL-divergence term.) Since both $p(\\theta)$ and $q(\\theta;\\phi)$ are Gaussians, we can compute their KL-divergence in close-form. Compute the KL divergence, and complete the following code to compute it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpMT7TyIVl_C"
      },
      "source": [
        "# compute KL divergence between q(mu, sigma) and p(0, prior_std)\n",
        "def kl_divergence_(mu, sigma_uc, prior_std):\n",
        "    # fill this part\n",
        "    ################################################################################\n",
        "\n",
        "    ################################################################################\n",
        "\n",
        "def kl_divergence(var_params, prior_std):\n",
        "    kld = kl_divergence_(var_params['W1_mu'], var_params['W1_sigma_uc'], prior_std)\n",
        "    kld += kl_divergence_(var_params['b1_mu'], var_params['b1_sigma_uc'], prior_std)\n",
        "    kld += kl_divergence_(var_params['W2_mu'], var_params['W2_sigma_uc'], prior_std)\n",
        "    kld += kl_divergence_(var_params['b2_mu'], var_params['b2_sigma_uc'], prior_std)\n",
        "    kld += kl_divergence_(var_params['W3_mu'], var_params['W3_sigma_uc'], prior_std)\n",
        "    kld += kl_divergence_(var_params['b3_mu'], var_params['b3_sigma_uc'], prior_std)\n",
        "    return kld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXQfIpc6XFTz"
      },
      "source": [
        "(c) (30pts) The expected likelihood term is approximated with reparameterization trick. In this homework, we will implement the local reparameterization trick (https://arxiv.org/abs/1506.02557) that was covered in the class. Complete the code below doing forward passes with locally reparameterized samples from the variational distributions. You won't get any score if you implement the vanilla reparameterization trick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_MBfNbDZPDx"
      },
      "source": [
        "def MLP_local_reparam(key, var_params, x):\n",
        "\n",
        "    # takes W_mu, W_sigma_uc, b_mu, b_sigma_uc and draw samples from\n",
        "    # y ~ Wx + b where W ~ N(W_mu, W_sigma) and b ~ N(b_mu, b_sigma)\n",
        "    def local_reparam_linear(key, W_mu, W_sigma_uc, b_mu, b_sigma_uc, x):\n",
        "        # fill this part\n",
        "        ################################################################################\n",
        "\n",
        "        ################################################################################\n",
        "        return key, output\n",
        "\n",
        "    key, y = local_reparam_linear(\n",
        "        key, var_params['W1_mu'], var_params['W1_sigma_uc'],\n",
        "        var_params['b1_mu'], var_params['b1_sigma_uc'], x[...,None]\n",
        "        )\n",
        "    y = jax.nn.swish(y)\n",
        "        \n",
        "    key, y = local_reparam_linear(\n",
        "        key, var_params['W2_mu'], var_params['W2_sigma_uc'],\n",
        "        var_params['b2_mu'], var_params['b2_sigma_uc'], y\n",
        "        )\n",
        "    y = jax.nn.swish(y)\n",
        "\n",
        "    key, y = local_reparam_linear(\n",
        "        key, var_params['W3_mu'], var_params['W3_sigma_uc'],\n",
        "        var_params['b3_mu'], var_params['b3_sigma_uc'], y\n",
        "        )\n",
        "    \n",
        "    y_mean = y[...,0]\n",
        "    y_std = jax.nn.softplus(y[...,1])\n",
        "    return key, y_mean, y_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH-bullPcU7G"
      },
      "source": [
        "(d) (10pts) Using the kl-divergence and local reparametrization implemented above, complete the code below computing the negative ELBO. Then run the remaining codes to verify your implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18z02bwjcb6Y"
      },
      "source": [
        "# computes negative ELBO,\n",
        "# - E_q [log p(Y|X,theta)] + gamma * KL[q||p]\n",
        "def neg_elbo(key, var_params, x, y, prior_std, gamma):\n",
        "    # fill this part\n",
        "    ################################################################################\n",
        "\n",
        "    ################################################################################\n",
        "\n",
        "\n",
        "def neg_elbo_and_grad(key, var_params, x, y, prior_std, gamma):\n",
        "    grad_fn = jax.value_and_grad(neg_elbo, argnums=1)\n",
        "    key, key_ = jax.random.split(key, 2)\n",
        "    nelbo, grads = grad_fn(key, var_params, x, y, prior_std, gamma)\n",
        "    return key, nelbo, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsAV9m5ldeH3"
      },
      "source": [
        "Run the code below to see if your implementation is right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8WohoCgdhQ9"
      },
      "source": [
        "opt = optax.adam(1e-3)\n",
        "gamma = 0.1\n",
        "@jax.jit\n",
        "def bbb_train_step(key, var_params, opt_state):\n",
        "    key, nelbo, grads = neg_elbo_and_grad(\n",
        "        key, var_params, train_x, train_y, prior_std, gamma)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    var_params = optax.apply_updates(var_params, updates)\n",
        "    return key, nelbo, var_params, opt_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaYr2stnd3lk"
      },
      "source": [
        "key, var_params = init_var_params(key, num_hidden)\n",
        "opt_state = opt.init(var_params)\n",
        "num_steps = 10000\n",
        "for t in range(1, num_steps+1):\n",
        "    key, nelbo, var_params, opt_state = bbb_train_step(key, var_params, opt_state)\n",
        "    if t % 1000 == 0:\n",
        "        print(f'step {t}, neg elbo {nelbo:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW3F9EKEediH"
      },
      "source": [
        "# For evaluation, only one parameter is required for each test,\n",
        "# so we use vanilla reparameterization trick.\n",
        "def MLP_reparam(key, var_params, x):\n",
        "    \n",
        "    def reparam_linear(key, W_mu, W_sigma_uc, b_mu, b_sigma_uc, x):        \n",
        "        W_sigma = jax.nn.softplus(W_sigma_uc)\n",
        "        b_sigma = jax.nn.softplus(b_sigma_uc)\n",
        "\n",
        "        key, key_ = jax.random.split(key, 2)\n",
        "        eps = jax.random.normal(key_, W_sigma.shape)\n",
        "        W = W_mu + W_sigma * eps\n",
        "\n",
        "        key, key_ = jax.random.split(key, 2)\n",
        "        eps = jax.random.normal(key_, b_sigma.shape)\n",
        "        b = b_mu + b_sigma * eps\n",
        "\n",
        "        return key, jnp.dot(x, W) + b\n",
        "\n",
        "    key, y = reparam_linear(\n",
        "        key, var_params['W1_mu'], var_params['W1_sigma_uc'],\n",
        "        var_params['b1_mu'], var_params['b1_sigma_uc'], x[...,None]\n",
        "        )\n",
        "    y = jax.nn.swish(y)\n",
        "        \n",
        "    key, y = reparam_linear(\n",
        "        key, var_params['W2_mu'], var_params['W2_sigma_uc'],\n",
        "        var_params['b2_mu'], var_params['b2_sigma_uc'], y\n",
        "        )\n",
        "    y = jax.nn.swish(y)\n",
        "\n",
        "    key, y = reparam_linear(\n",
        "        key, var_params['W3_mu'], var_params['W3_sigma_uc'],\n",
        "        var_params['b3_mu'], var_params['b3_sigma_uc'], y\n",
        "        )\n",
        "    \n",
        "    y_mean = y[...,0]\n",
        "    y_std = jax.nn.softplus(y[...,1])\n",
        "    return key, y_mean, y_std\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(121)\n",
        "test_x = jnp.linspace(-1.1, 1.1, 100)\n",
        "y_means, y_stds = [], []\n",
        "num_samples = 50\n",
        "for i in range(num_samples):\n",
        "    key, y_mean, y_std = MLP_reparam(key, var_params, test_x)\n",
        "    plt.plot(test_x, y_mean, alpha=0.2)\n",
        "    y_means.append(y_mean)    \n",
        "    y_stds.append(y_std)\n",
        "plt.plot(train_x, train_y, 'rx', label='training data')\n",
        "plt.legend()\n",
        "\n",
        "y_means = jnp.stack(y_means)\n",
        "y_stds = jnp.stack(y_stds)\n",
        "y_mean, y_std = compute_pred_mean_and_std(y_means, y_stds)\n",
        "plt.subplot(122)\n",
        "plt.fill_between(test_x, y_mean-2*y_std, y_mean+2*y_std, alpha=0.2, facecolor='skyblue', edgecolor=None)\n",
        "plt.fill_between(test_x, y_mean-y_std, y_mean+y_std, alpha=0.4, facecolor='skyblue', edgecolor=None)\n",
        "plt.plot(test_x, y_mean, label='prediction')\n",
        "plt.plot(train_x, train_y, 'rx', label='training data')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}