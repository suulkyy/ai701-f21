\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,enumerate,dsfont,cancel,amssymb,amsthm}
\usepackage[top=2cm, bottom=3cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage[pagebackref=true,linkcolor=blue,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\title{AI701 Assignment 1}
\author{Sultan Rizky Hikmawan Madjid (20214196)}
\date{Due: September 21, 2021}

\begin{document}
	
\maketitle

% Problem 1
\begin{enumerate}
\item For a random variable $X$ denoting the number of heads among $n$ coin tosses which follows the binomial distribution with parameter $\theta$,
\begin{equation*}
	\mathbb{P}(X=x|\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}
\end{equation*}
\begin{enumerate}[(a)]
	% Section (a) (5 points)
	\item Given that $\theta$ itself is a random variable following the beta distribution with Probability Density Function (PDF) with parameters $a,b > 0$,
	\begin{equation*}
		f(\theta;a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \theta^{a-1} (1-\theta)^{b-1}\mathds{1}_{0 \leq \theta \leq 1}
	\end{equation*}
	for gamma function $\Gamma(t) = \int_{0}^{\infty} z^{t-1}e^{-z} \,\mathrm{d}z$, the marginal likelihood of $x$ under this setting can be expressed as
	\begin{equation*}
		\begin{aligned}
			\mathbb{P}(X = x;a,b) &= \int_{0}^{1} \mathbb{P}(X=x|\theta) f(\theta;a,b) \,\mathrm{d}\theta \\
			&= \binom{n}{x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \int_{0}^{1} \theta^{x+a-1} (1-\theta)^{n+b-x-1} \,\mathrm{d}\theta \\
			&= \binom{n}{x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \biggl(\cancelto{0}{\biggl[\frac{\theta^{x+a} (1-\theta)^{n+b-x-1}}{x+a}\biggr]_0^1} + \frac{n+b-x-1}{x+a} \int_{0}^{1} \theta^{x+a} (1-\theta)^{n+b-x-2} \,\mathrm{d}\theta\biggr) \\
			&= \binom{n}{x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \frac{(n+b-x-1) \cdot (n+b-x-2) \cdot ... \cdot 1}{(a+b+n-2) \cdot (a+b+n-3) \cdot ... \cdot (x+a)} \int_{0}^{1} \theta^{a+n+b-2} \,\mathrm{d}\theta \\
			&= \binom{n}{x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \frac{(n+b-x-1) \cdot (n+b-x-2) \cdot ... \cdot 1}{(a+b+n-1) \cdot (a+b+n-2) \cdot ... \cdot (x+a)} \\
			&= \binom{n}{x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \frac{\Gamma(n+b-x)\Gamma(x+a)}{\Gamma(a+b+n)}
		\end{aligned}
	\end{equation*}
	% Section (b) (10 points)
	\item Assuming that we toss the coin $n=10$ times and see $x=9$ heads, for two prior distributions $\mathcal{M}_1$ and $\mathcal{M}_2$ for $\theta$ defined as:
	\begin{equation*}
		\mathcal{M}_1 : \mathbb{P}(\theta) = \delta_{1/2}(\theta), \quad \mathcal{M}_2 : f(\theta;a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \theta^{a-1} (1-\theta)^{b-1} \mathds{1}_{\{0 \leq \theta \leq 1\}}.
	\end{equation*}
	Fixing $a=b=1$, we'll have that Bayes factor
	\begin{equation*}
		\begin{aligned}
			\frac{\mathbb{P}(X=x|\mathcal{M}_1)}{\mathbb{P}(X=x|\mathcal{M}_2)} &= \frac{\int {\mathbb{P}(X=x|\theta) \mathbb{P}(\theta|\mathcal{M}_1})\,\mathrm{d}\theta}{\int {\mathbb{P}(X=x|\theta) \mathbb{P}(\theta|\mathcal{M}_2})\,\mathrm{d}\theta} \\
			&= \frac{\binom{n}{x}(1/2)^x (1-1/2)^{n-x}}{\binom{n}{x} \int_{0}^{1} \theta^x (1-\theta)^{n-x}\,\mathrm{d}\theta} \\
			&= \frac{(1/2)^n \Gamma(n+2)}{\Gamma(x+1)\Gamma(n-x+1)} \\
			&= \frac{(1/2)^{10} (10+1)!}{9!(10-9)!} = \frac{55}{512} = 0.107421875 \\
		\end{aligned}
	\end{equation*}
\end{enumerate}
\pagebreak
% Problem 2
\item For a set $E$ and its collection of subsets $\mathcal{A}$, defining $\sigma(\mathcal{A})$ as the smallest $\sigma$-algebra containing $\mathcal{A}$ or the intersection of all $\sigma$-algebras containing $\mathcal{A}$,
\begin{enumerate}[(a)]
	% Section (a) (5 points)
	\item Given $E = \{1,2,3,4\}$ and $\mathcal{A}=\{\{1\},\{2\}\}$, from the definition of $\sigma$-algebra itself, given $\mathcal{A} = \{A, B\}$, we have that $\sigma(\mathcal{A}) = \{\varnothing, E, A, B, E\setminus A, E\setminus B, A \cup B, E\setminus (A \cup B)\}$. In other words, 
	\begin{equation*}
		\sigma(\mathcal{A}) = \{\varnothing, \{1\}, \{2\}, \{1,2\}, \{3,4\}, \{2,3,4\}, \{1,3,4\}, \{1,2,3,4\}\} 
	\end{equation*}
	% Section (b) (10 points)
	\item For a set $E$ and its collection of subsets $\mathcal{A},\mathcal{C}$,
	\begin{enumerate}[1.]
		% Subsection 1
		\item \label{2(b)(i)} \textit{Proof.} Given $\mathcal{A} \subset \mathcal{C}$, from the definition of $\sigma$-algebra generated by $\mathcal{C}$, $\sigma(\mathcal{C})$, which is the smallest $\sigma$-algebra on $E$ that contains $\mathcal{C}$, we have that $\mathcal{C} \subset \sigma(\mathcal{C})$. Intuitively, this implies that $\mathcal{A} \subset \mathcal{C} \subset \sigma(\mathcal{C})$, or in short, $\mathcal{A} \subset \sigma(\mathcal{C})$.
		In other words, $\sigma(\mathcal{C})$ is a $\sigma$-algebra which contains the sets in $\mathcal{A}$.
		Thus, from the definition of $\sigma$-algebra generated by $\mathcal{A}$, which is the smallest $\sigma$-algebra on $E$ that contains $\mathcal{A}$, we have that $\mathcal{A} \subset \sigma(\mathcal{A}) \subset \sigma(\mathcal{C})$, or in short $\sigma(\mathcal{A}) \subset \sigma(\mathcal{C})$. \qedsymbol
		% Subsection 2
		\item \label{2(b)(ii)} \textit{Proof.} Given $\mathcal{A} \subset \sigma(\mathcal{C})$ and $\mathcal{C} \subset \sigma(\mathcal{A})$, it can be noticed that given $\mathcal{A} \subset \sigma(\mathcal{C})$, from the definition of $\sigma$-algebra generated by $\mathcal{A}$, which is the smallest $\sigma$-algebra on $E$ that contains $\mathcal{A}$, we have that $\mathcal{A} \subset \sigma(\mathcal{A}) \subset \sigma(\mathcal{C})$, or in short, $\sigma(\mathcal{A}) \subset \sigma(\mathcal{C})$.
		Similarly, given $\mathcal{C} \subset \sigma(\mathcal{A})$, from the definition of $\sigma$-algebra generated by $\mathcal{C}$, which is the smallest $\sigma$-algebra on $E$ that contains $\mathcal{C}$, we have that $\mathcal{C} \subset \sigma(\mathcal{C}) \subset \sigma(\mathcal{A})$, or in short, $\sigma(\mathcal{C}) \subset \sigma(\mathcal{A})$.
		Since we have $\sigma(\mathcal{C}) \subset \sigma(\mathcal{A})$ and $\sigma(\mathcal{A}) \subset \sigma(\mathcal{C})$, therefore, it implies that $\sigma(\mathcal{A}) = \sigma(\mathcal{C})$. \qedsymbol
		% Subsection 3
		\item \label{2(b)(iii)} \textit{Proof.} Given $\mathcal{A} \subset \mathcal{C} \subset \sigma(\mathcal{A})$, from \ref{2(b)(i)}, we have that given $\mathcal{A} \subset \mathcal{C}$, $\sigma(\mathcal{A}) \subset \sigma(\mathcal{C})$, which further implies that $\mathcal{A} \subset \sigma(\mathcal{A}) \subset \sigma(\mathcal{C})$. Now, from \ref{2(b)(ii)}, since we have both $\mathcal{A} \subset \sigma(\mathcal{C})$ and $\mathcal{C} \subset \sigma(\mathcal{A})$, it means that $\sigma(\mathcal{A}) = \sigma(\mathcal{C})$, completing our proof. \qedsymbol
	\end{enumerate}
\end{enumerate}
\pagebreak
% Problem 3
\item For Moment Generating Function (\textsc{mgf}) of a $\mathbb{R}$-valued random variable $X$ defined as
\begin{equation*}
	M_X(t)=\mathbb{E}\!\left[e^{tX}\right]
\end{equation*}
\begin{enumerate}[(a)]
	% Section (a) (5 points)
	\item \label{3(a)} For a standard normal random variable $X$ with \textsc{pdf} $f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$, we have that it's Moment Generating Function (\textsc{mgf}) is given by:
	\begin{equation*}
		\begin{aligned}
			M_X(t) &= \mathbb{E}\!\left[e^{tX}\right] = \int_{-\infty}^{\infty} e^{tx}f_X(x)\,\mathrm{d}x \\
			&= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{tx-\frac{x^2}{2}}\,\mathrm{d}x \\
			&= e^{\frac{t^2}{2}} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(x-t)^2}\,\mathrm{d}x \\
			&= e^{\frac{t^2}{2}}
		\end{aligned}
	\end{equation*}
	% Section (b) (5 points)
	\item Defining $X_1,...,X_n$ for $n \in \mathbb{N}$ as i.i.d. $\mathbb{R}$-valued random variables with $M_{X_1}(t)$ defined as the \textsc{mgf} of $X_1$, defining $Y_n := c(X_1+X_2+...+X_n)$, it's \textsc{mgf} can be mathematically expressed as:
	\begin{equation*}
		\begin{aligned}
			M_{Y_n}(t) &= \mathbb{E}\!\left[e^{tY_n}\right] = \mathbb{E}\!\left[e^{tc(X_1+X_2+...+X_n)}\right] \\
			&= \prod_{i=1}^{n} \mathbb{E}\!\left[e^{tcX_i} \right ]  = \left(\mathbb{E}\!\left[e^{tcX_1}\!\right]\right)^n = \left(\mathbb{E}\!\left[e^{ctX_1}\!\right]\right)^n \\
			&= \left(M_{X_1}(ct)\right)^n \\
		\end{aligned}
	\end{equation*}
	% Section (c) (15 points)
	\item \textit{Proof.} For i.i.d. $\mathbb{R}$-valued random variables $X_i$ with mean $\mu$ and variance $\sigma^2$, we first define for $n \in \mathbb{N}$ that
	\begin{equation*}
		Y_i:=\frac{X_i-\mu}{\sigma},\quad Z_n:=\frac{1}{\sqrt{n}}\sum_{i=1}^{n} Y_i.
	\end{equation*}
	Now, to simplify our calculations, we are going to use random variables $Y_i$ which satisfy $\mathbb{E}\!\left[Y_i\right] = \mathbb{E}\!\left[\frac{X_i-\mu}{\sigma}\right] = 0$ and $\mathbb{E}\!\left[Y_i^2\right]=\mathbb{E}\!\left[\frac{\left(X_i-\mu\right)^2}{\sigma^2}\right]=\frac{\mathrm{Var}\left[X_i\right]}{\sigma^2}=1$. The \textsc{mgf} of $Z_n$ can be calculated as:
	\begin{equation*}
		\begin{aligned}
			M_{Z_n}(t) &= \mathbb{E}\!\left[e^{tZ_n}\!\right] \\
			&= \mathbb{E}\!\left[\mathrm{exp}\!\left(\frac{t}{\sqrt{n}}\left(\sum_{i=1}^{n}Y_i\right)\right)\!\right] \\
			&= \prod_{i=1}^{n} \mathbb{E}\!\left[e^{tY_i/\sqrt{n}}\right] = \left(M_{Y_i}\left(\frac{t}{\sqrt{n}}\right)\right)^n \\
		\end{aligned}
	\end{equation*}
	From Taylor Series expansion, by denoting $o(t^2)$ as a function that satisfies $\displaystyle \lim_{t \to 0} o(t^2)/t^2 = 0$, it is possible to express $M_{Z_n}(t)$ as
	\begin{equation*}
		\begin{aligned}
			M_{Z_n}(t) &= \left(M_{Y_i}\left(\frac{t}{\sqrt{n}}\right)\right)^n = \left(M_{Y_i}(0)+\frac{\mathbb{E}\!\left[Y_i\right]t}{\sqrt{n}}+\frac{\mathbb{E}\!\left[Y_i^2\right]t^2}{2n}+o\left(\frac{t^2}{n}\right)\right)^n \\
			&= \left(1+\frac{t^2}{2n}+o\left(\frac{t^2}{n}\right)\right)^n
		\end{aligned}
	\end{equation*}
	Now, taking the limit as $n \to \infty$ and neglecting the higher-order terms represented by $o(\frac{t^2}{n})$ since it converges to $0$ faster than the other existing terms, it is possible to deduce that
	\begin{equation*}
		\begin{aligned}
			\lim_{n \to \infty} M_{Z_n}(t) &= \lim_{n \to \infty} \left(1+\frac{t^2}{2n}\right)^n \\
			&= e^{\frac{t^2}{2}} 
		\end{aligned}
	\end{equation*}
	which is the \textsc{mgf} of a standard normal distribution as derived in \ref{3(a)}. Thus, $Z_n$ converges in distribution to a standard normal distribution as $n \to \infty$. \qedsymbol
\end{enumerate}
\pagebreak
% Problem 4
\item For a random variable $X$ with finite mean $\mu$ and variance $\sigma^2$ and its i.i.d. copies $X_1,...,X_n$, defining estimator for $\sigma$:
\begin{equation*}
	\hat{\sigma}_n^2 := \frac{1}{n} \sum_{i=1}^{n} \left(X_i-\bar{X}_n\right)^2, \quad \bar{X}_n := \frac{1}{n} \sum_{i=1}^n X_i.
\end{equation*}
\begin{enumerate}[(a)]
	% Section (a) (10 points)
	\item \label{4(a)} \textit{Proof.} From the Weak Law of Large Numbers, it can be noticed that as $n \to \infty$, we have that $\bar{X}_n := \frac{1}{n} \sum_{i=1}^{n} X_i \overset{p}{\to} \mathbb{E}\!\left[X\right]$ and similarly, $\frac{1}{n} \sum_{i=1}^{n} X_i^2 \overset{p}{\to} \mathbb{E}\!\left[X^2\right]$. 
	From the fact that
	\begin{equation*}
		\hat{\sigma}_n^2 := \frac{1}{n} \sum_{i=1}^{n} \left(X_i-\bar{X}_n\right)^2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2 - \bar{X}_n^2
	\end{equation*}
	Using both propositions deduced from the Weak Law of Large Numbers and the equation above, it can be implied that as $n \to \infty$, $\hat{\sigma}_n^2 \overset{p}{\to} \mathbb{E}\!\left[X^2\right]- \left(\mathbb{E}\!\left[X\right]\right)^2 = \mathrm{Var}\!\left[X\right]= \sigma^2$ or in short, $\hat{\sigma}_n^2 \overset{p}{\to} \sigma^2$. Thus, $\hat{\sigma}_n^2$ is consistent. \qedsymbol
	% Section (b) (10 points)
	\item \label{4(b)} \textit{Proof.} Using the expanded form of $\hat{\sigma}_n^2$ derived in \ref{4(a)}, it is possible to express that
	\begin{equation*}
		\begin{aligned}
			\mathbb{E}\!\left[\hat{\sigma}_n^2\right] &= \frac{1}{n} \sum_{i=1}^n \mathbb{E}\!\left[X_i^2\right] - \mathbb{E}\!\left[\bar{X}_n^2\right] \\
			&= \frac{1}{n} \sum_{i=1}^n \left(\mathrm{Var}\!\left[X_i\right]+\left(\mathbb{E}\!\left[X_i\right]\right)^2\right) - \left(\mathrm{Var}\!\left[\bar{X}_n\right]+\left(\mathbb{E}\!\left[\bar{X}_n\right]\right)^2\right) \\
			&= \sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2 \\
			&= \frac{n-1}{n} \sigma^2 \neq \sigma^2
		\end{aligned}
	\end{equation*}
	Due to the fact that $\mathbb{E}\!\left[\hat{\sigma}_n^2\right] \neq \sigma^2$, $\hat{\sigma}_n^2$ is biased. \qedsymbol
	% Section (c) (5 points)
	\item In order to make $\hat{\sigma}_n^2$ unbiased, it is possible to redefine the estimator of $\sigma$ as follows:
	\begin{equation*}
		\hat{\sigma}_n^2 := \frac{1}{n-1} \sum_{i=1}^{n} \left(X_i-\bar{X}_n\right)^2, \quad \bar{X}_n := \frac{1}{n} \sum_{i=1}^n X_i.
	\end{equation*}
	By this way, we are going to have an estimator $\hat{\sigma}_n^2$ which is \textit{consistent} from the fact that given $n \to \infty$, $\hat{\sigma}_n^2 = \frac{n}{n-1} \left(\frac{1}{n} \sum_{i=1}^{n} X_i^2 - \bar{X}_n^2\right) \overset{p}{\to} (1)\cdot\sigma^2$, or in short, $\hat{\sigma}_n^2 \overset{p}{\to} \sigma^2$ and \textit{unbiased}, using the fact that 
	\begin{equation*}
		\begin{aligned}
			\mathbb{E}\!\left[\hat{\sigma}_n^2\right] &= \frac{n}{n-1} \left(\sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2\right) \\
			&= \sigma^2
		\end{aligned}
	\end{equation*}
\end{enumerate}

% Problem 5 (20 points)
\pagebreak
\item \textit{Proof.} For a sequence of $\mathbb{R}$-valued random variables $\left(X_n\right)_{n \geq 1}$ with distribution functions $\left(F_n\right)_{n \geq 1}$ and a $\mathbb{R}$-valued random variable $X$ with distribution $F$, given $X_n \overset{d}{\to} c$, where $c$ is a constant, from the definition of Convergence in Distribution that is being normally presented by the notation $X_n \overset{d}{\to} X$, where for all $x \in \mathbb{R}$, $\displaystyle \lim_{n \to \infty} F_n(x) = F(x)$, given $X=c$, we'll have that $F(x) = 0$ if $x < c$ and $F(x) = 1$ if $x \geq c$.
Here, $F(x)$ is continuous everywhere except at $c$. 
This implies that for any $\epsilon > 0$, we have that
\begin{equation*}
	\begin{aligned}
		\lim_{n \to \infty} F_n(c-\epsilon) &= \lim_{n \to \infty} \left[\mathbb{P}\!\left( X_n \leq c - \epsilon\right)\right] = 0, \\
		\lim_{n \to \infty} F_n\!\left(c+\frac{\epsilon}{2}\right) &= \lim_{n \to \infty} \left[\mathbb{P}\!\left( X_n \leq c + \frac{\epsilon}{2} \right)\right] = 1. 
	\end{aligned}
\end{equation*}
Naturally, we have that $\displaystyle \lim_{n \to \infty}\mathbb{P}\!\left(\left| X_n - c \right|> \epsilon\right) \geq 0$ for every $\epsilon > 0$.
However, it can be noticed that
\begin{equation*}
	\begin{aligned}
		\lim_{n \to \infty} \mathbb{P}\!\left(\left| X_n - c \right|> \epsilon\right) &= \lim_{n \to \infty} \left[\mathbb{P}\!\left( X_n < c - \epsilon\right) + \mathbb{P}\!\left( X_n > c + \epsilon\right)\right] \\
		&= \lim_{n \to \infty}\left[\mathbb{P}\!\left( X_n < c - \epsilon\right)\right] + \lim_{n \to \infty}\left[\mathbb{P}\!\left( X_n > c + \epsilon\right)\right] \\
		% &\leq \lim_{n \to \infty} F_n\!\left(c - \epsilon\right) + \lim_{n \to \infty} \mathbb{P}\!\left( X_n > c + \epsilon\right) \\
		&= 0 + \lim_{n \to \infty} \left[\mathbb{P}\!\left( X_n > c + \epsilon\right)\right] \\
		&\leq \lim_{n \to \infty}\left[\mathbb{P}\!\left( X_n > c + \frac{\epsilon}{2}\right)\right] \\
		&= 1 - \lim_{n \to \infty} \left[F_n\!\left(c+\frac{\epsilon}{2}\right)\right] \\
		&= 0
	\end{aligned}
\end{equation*}
Therefore, from both inequalities, we have that $\displaystyle \lim_{n \to \infty} \mathbb{P}\!\left(\left| X_n - c \right|> \epsilon\right) = 0$ for every $\epsilon > 0$, or equivalently, $X_n \overset{p}{\to} c$. \qedsymbol
\end{enumerate}

\end{document}